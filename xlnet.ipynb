{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "from transformers.modeling_utils import SequenceSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "train = create_folds(train, num_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>25ca8f498</td>\n",
       "      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>When you think of dinosaurs and where they liv...</td>\n",
       "      <td>1.711390</td>\n",
       "      <td>0.646900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>2c26db523</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>So what is a solid? Solids are usually hard be...</td>\n",
       "      <td>0.189476</td>\n",
       "      <td>0.535648</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>cd19e2350</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:The_E...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>The second state of matter we will discuss is ...</td>\n",
       "      <td>0.255209</td>\n",
       "      <td>0.483866</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>15e2e9e7a</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Geometry_for_Ele...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Solids are shapes that you can actually touch....</td>\n",
       "      <td>-0.215279</td>\n",
       "      <td>0.514128</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>5b990ba77</td>\n",
       "      <td>https://en.wikibooks.org/wiki/Wikijunior:Biolo...</td>\n",
       "      <td>CC BY-SA 3.0</td>\n",
       "      <td>Animals are made of many cells. They eat thing...</td>\n",
       "      <td>0.300779</td>\n",
       "      <td>0.512379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2834 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          url_legal  \\\n",
       "0     c12129c31                                                NaN   \n",
       "1     85aa80a4c                                                NaN   \n",
       "2     b69ac6792                                                NaN   \n",
       "3     dd1000b26                                                NaN   \n",
       "4     37c1b32fb                                                NaN   \n",
       "...         ...                                                ...   \n",
       "2829  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n",
       "2830  2c26db523  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2831  cd19e2350  https://en.wikibooks.org/wiki/Wikijunior:The_E...   \n",
       "2832  15e2e9e7a  https://en.wikibooks.org/wiki/Geometry_for_Ele...   \n",
       "2833  5b990ba77  https://en.wikibooks.org/wiki/Wikijunior:Biolo...   \n",
       "\n",
       "           license                                            excerpt  \\\n",
       "0              NaN  When the young people returned to the ballroom...   \n",
       "1              NaN  All through dinner time, Mrs. Fayre was somewh...   \n",
       "2              NaN  As Roger had predicted, the snow departed as q...   \n",
       "3              NaN  And outside before the palace a great garden w...   \n",
       "4              NaN  Once upon a time there were Three Bears who li...   \n",
       "...            ...                                                ...   \n",
       "2829  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n",
       "2830  CC BY-SA 3.0  So what is a solid? Solids are usually hard be...   \n",
       "2831  CC BY-SA 3.0  The second state of matter we will discuss is ...   \n",
       "2832  CC BY-SA 3.0  Solids are shapes that you can actually touch....   \n",
       "2833  CC BY-SA 3.0  Animals are made of many cells. They eat thing...   \n",
       "\n",
       "        target  standard_error  kfold  \n",
       "0    -0.340259        0.464009      3  \n",
       "1    -0.315372        0.480805      2  \n",
       "2    -0.580118        0.476676      3  \n",
       "3    -1.054013        0.450007      3  \n",
       "4     0.247197        0.510845      3  \n",
       "...        ...             ...    ...  \n",
       "2829  1.711390        0.646900      1  \n",
       "2830  0.189476        0.535648      2  \n",
       "2831  0.255209        0.483866      3  \n",
       "2832 -0.215279        0.514128      2  \n",
       "2833  0.300779        0.512379      1  \n",
       "\n",
       "[2834 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvDataset(Dataset):\n",
    "    def __init__(self, df_data):\n",
    "        self.data = copy.deepcopy(df_data)\n",
    "        self.data['excerpt'] = self.data['excerpt'].map(lambda x : x.replace('\\n', ' '))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        if 'target' in self.data.columns:\n",
    "            self.targets = self.data.target.values.tolist()\n",
    "            return {'excerpts': self.excerpts[index], 'targets': self.targets[index]}\n",
    "        else:\n",
    "            return {'excerpts': self.excerpts[index]}\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, tokenizer, max_len=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __call__(self, batch):\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        data_str =[item['excerpts'] for item in batch]\n",
    "        data_batch = self.tokenizer.batch_encode_plus(data_str, \n",
    "                                                        padding='longest', \n",
    "                                                        max_length=self.max_len,\n",
    "                                                        truncation=True, \n",
    "                                                        return_tensors='pt')\n",
    "        \n",
    "        if 'targets' in batch[0]:\n",
    "            targets =torch.tensor([item['targets'] for item in batch])#,dtype=torch.double)\n",
    "            \n",
    "            return {'input_ids': data_batch.input_ids,\n",
    "                #'token_type_ids': data_batch.token_type_ids,\n",
    "                'attention_mask': data_batch.attention_mask,\n",
    "                'label':targets\n",
    "                   }\n",
    "        \n",
    "\n",
    "\n",
    "        return {'input_ids': data_batch.input_ids,\n",
    "                #'token_type_ids': data_batch.token_type_ids,\n",
    "                'attention_mask': data_batch.attention_mask}\n",
    "        \n",
    "\n",
    "def get_dataloader(dataset, batch_generator, batch_size=4, shuffle=True):\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=shuffle, \n",
    "                             collate_fn=batch_generator,\n",
    "                             num_workers=4,\n",
    "                             pin_memory=True)\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.clmodel = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.sequence_summary = SequenceSummary(config)  \n",
    "        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.clmodel(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        output = self.sequence_summary(sequence_output)\n",
    "        logits = self.regressor(output)\n",
    "\n",
    "    \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # regression task\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "            #loss = loss_fn(logits, labels.view(-1))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lamb(Optimizer):\n",
    "    # Reference code: https://github.com/cybertronai/pytorch-lamb\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0,\n",
    "        clamp_value: float = 10,\n",
    "        adam: bool = False,\n",
    "        debias: bool = False,\n",
    "    ):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if eps < 0.0:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 0: {}'.format(betas[0])\n",
    "            )\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 1: {}'.format(betas[1])\n",
    "            )\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(\n",
    "                'Invalid weight_decay value: {}'.format(weight_decay)\n",
    "            )\n",
    "        if clamp_value < 0.0:\n",
    "            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.clamp_value = clamp_value\n",
    "        self.adam = adam\n",
    "        self.debias = debias\n",
    "\n",
    "        super(Lamb, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    msg = (\n",
    "                        'Lamb does not support sparse gradients, '\n",
    "                        'please consider SparseAdam instead'\n",
    "                    )\n",
    "                    raise RuntimeError(msg)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Paper v3 does not use debiasing.\n",
    "                if self.debias:\n",
    "                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n",
    "                    bias_correction /= 1 - beta1 ** state['step']\n",
    "                else:\n",
    "                    bias_correction = 1\n",
    "\n",
    "                # Apply bias to lr to avoid broadcast.\n",
    "                step_size = group['lr'] * bias_correction\n",
    "\n",
    "                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n",
    "\n",
    "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
    "                if group['weight_decay'] != 0:\n",
    "                    adam_step.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                adam_norm = torch.norm(adam_step)\n",
    "                if weight_norm == 0 or adam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / adam_norm\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = adam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "                if self.adam:\n",
    "                    trust_ratio = 1\n",
    "\n",
    "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "    # differential learning rate and weight decay\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    learning_rate = 5e-5\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.01, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.01, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.01, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"clmodel\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_optimizer = list(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_name='../input/xlnet-large-cased/', num_labels=1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlnet-large-cased')\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    \n",
    "    ### add below ###\n",
    "    config.update({ \n",
    "        \"attention_probs_dropout_prob\": 0.0,\n",
    "        \"hidden_dropout_prob\": 0.0\n",
    "        })\n",
    "    \n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_optimizer(model, optimizer_name=\"AdamW\"):\n",
    "    optimizer_grouped_parameters = get_optimizer_params(model)\n",
    "    kwargs = {\n",
    "            'lr':5e-5,\n",
    "            'weight_decay':0.01,\n",
    "            # 'betas': (0.9, 0.98),\n",
    "            # 'eps': 1e-06\n",
    "    }\n",
    "    if optimizer_name == \"LAMB\":\n",
    "        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        from torch.optim import Adam\n",
    "        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n",
    "\n",
    "def make_scheduler(optimizer, decay_name='linear', t_max=None, warmup_steps=None):\n",
    "    if decay_name == 'step':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=[30, 60, 90],\n",
    "            gamma=0.1\n",
    "        )\n",
    "    elif decay_name == 'cosine':\n",
    "        scheduler = lrs.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=t_max\n",
    "        )\n",
    "    elif decay_name == \"cosine_warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    elif decay_name == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps, \n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "    fold=0\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_set=csvDataset(data)\n",
    "    train_generator = BatchGenerator(tokenizer)\n",
    "    train_loader = get_dataloader(train_set,train_generator,batch_size)\n",
    "    \n",
    "    valid_set=csvDataset(valid_set)\n",
    "    valid_generator = BatchGenerator(tokenizer)\n",
    "    valid_loader = get_dataloader(valid_set,valid_generator,batch_size//2)\n",
    "    \n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, log_interval=1, evaluate_interval=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.log_interval = log_interval\n",
    "        self.evaluate_interval = evaluate_interval\n",
    "        self.evaluator = Evaluator(self.model)\n",
    "        self.gradient_accumulation_steps = 4\n",
    "\n",
    "    def train(self, train_loader, valid_loader, epoch, \n",
    "              result_dict, tokenizer, fold):\n",
    "        count = 0\n",
    "        self.model.train()\n",
    "        device = 'cuda:1'\n",
    "        \n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "            input_ids, attention_mask, token_type_ids, labels = \\\n",
    "                input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "                #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "          \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            loss = loss / self.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if batch_idx % self.gradient_accumulation_steps:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.scheduler.step()\n",
    "    \n",
    "\n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                _s = str(len(str(len(train_loader.sampler))))\n",
    "                ret = [\n",
    "                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n",
    "                    'train_loss: {: >4.5f}'.format(loss),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "            \n",
    "            if batch_idx % self.evaluate_interval == 0:\n",
    "                result_dict = self.evaluator.evaluate(\n",
    "                    valid_loader, \n",
    "                    epoch, \n",
    "                    result_dict, \n",
    "                    tokenizer\n",
    "                )\n",
    "                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n",
    "                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n",
    "\n",
    "        result_dict['train_loss'].append(loss)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def worst_result(self):\n",
    "        ret = {\n",
    "            'loss':float('inf'),\n",
    "            'accuracy':0.0\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    def result_to_str(self, result):\n",
    "        ret = [\n",
    "            'epoch: {epoch:0>3}',\n",
    "            'loss: {loss: >4.2e}'\n",
    "        ]\n",
    "        for metric in self.evaluation_metrics:\n",
    "            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n",
    "        return ', '.join(ret).format(**result)\n",
    "\n",
    "    def save(self, result):\n",
    "        with open('result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "    \n",
    "    def load(self):\n",
    "        result = self.worst_result\n",
    "        if os.path.exists('result_dict.json'):\n",
    "            with open('result_dict.json', 'r') as f:\n",
    "                try:\n",
    "                    result = json.loads(f.read())\n",
    "                except:\n",
    "                    pass\n",
    "        return result\n",
    "\n",
    "    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n",
    "        device = 'cuda:1'        \n",
    "        losses = AverageMeter()\n",
    "\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "                input_ids, attention_mask, token_type_ids, labels = \\\n",
    "                input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "                \n",
    "              \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] valid_loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "epochs = 8\n",
    "max_len = 1024\n",
    "batch_size = 16\n",
    "\n",
    "#model, tokenizer = make_model(model_name='xlnet-large-cased', num_labels=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from  transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "#model = XLNetForSequenceClassification.from_pretrained('xlnet-large-cased', num_labels = 1)\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels = 1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-large')\n",
    "device= \"cuda:1\"\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model5.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "    fold=0\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_set=csvDataset(data)\n",
    "    train_generator = BatchGenerator(tokenizer)\n",
    "    train_loader = get_dataloader(train_set,train_generator,batch_size)\n",
    "    \n",
    "    valid_set=csvDataset(valid_set)\n",
    "    valid_generator = BatchGenerator(tokenizer)\n",
    "    valid_loader = get_dataloader(valid_set,valid_generator,batch_size//2)\n",
    "    \n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = make_loader(\n",
    "    train, tokenizer, max_len=max_len,\n",
    "    batch_size=batch_size, fold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 16\n",
    "BATCH_SIZE = 8\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "        input_ids, attention_mask, labels = batch_data['input_ids'], \\\n",
    "        batch_data['attention_mask'], batch_data['label']\n",
    "        input_ids, attention_mask, labels = \\\n",
    "            input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # preds = preds.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        prediction = outputs[1].cpu().detach().numpy()\n",
    "        accuracy=mean_squared_error(labels,prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        counter = counter + 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "----------\n",
      "Train loss 0.015924288145674582 Train accuracy 0.01592428814044243\n",
      "Epoch 2/16\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 999\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,     \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(train)\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_loss} Train accuracy {train_acc}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)    \n",
    "\n",
    "    if train_acc < best_accuracy:\n",
    "        torch.save(model.state_dict(), \"model5.bin\")\n",
    "        best_accuracy = train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.03069946367105239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# test_set=csvDataset(test)\n",
    "# generator = BatchGenerator(tokenizer)\n",
    "# test_loader = get_dataloader(test_set,generator,batch_size)\n",
    "\n",
    "# input_ids, attention_mask, token_type_ids = test_batch['input_ids'], \\\n",
    "# test_batch['attention_mask'], test_batch['token_type_ids']\n",
    "# input_ids, attention_mask, token_type_ids = \\\n",
    "#     input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask= test_batch['input_ids'], \\\n",
    "test_batch['attention_mask']\n",
    "input_ids, attention_mask = \\\n",
    "    input_ids.to(device), attention_mask.to(device)\n",
    "    #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1450],\n",
       "        [-0.0205],\n",
       "        [-1.3368],\n",
       "        [-1.3012],\n",
       "        [-0.6730],\n",
       "        [-0.8062],\n",
       "        [-1.4579],\n",
       "        [-0.0270],\n",
       "        [-0.2827],\n",
       "        [-2.2349],\n",
       "        [-0.3861],\n",
       "        [-0.5837],\n",
       "        [-1.8859],\n",
       "        [-1.2470],\n",
       "        [ 0.9654],\n",
       "        [-3.3354]], device='cuda:1', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1969, -0.0868, -1.2149, -1.2125, -0.8335, -0.7310, -1.3928,  0.0215,\n",
       "        -0.2370, -2.2534, -0.4163, -0.3716, -1.7088, -1.0768,  1.0909, -3.3092])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassificationOutput(loss=None, logits=tensor([[-1.8539],\n",
       "        [-1.7061],\n",
       "        [-1.5374],\n",
       "        [-1.6646],\n",
       "        [-2.0674],\n",
       "        [-0.8819],\n",
       "        [-0.0977],\n",
       "        [-2.3524],\n",
       "        [-0.1410],\n",
       "        [-0.3518],\n",
       "        [ 1.2315],\n",
       "        [-2.3641],\n",
       "        [ 1.5429],\n",
       "        [ 1.4368],\n",
       "        [-0.1662],\n",
       "        [-1.1045]], device='cuda:1', grad_fn=<AddmmBackward>), mems=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jisu_env",
   "language": "python",
   "name": "jisu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
