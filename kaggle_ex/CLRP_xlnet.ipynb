{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange\n",
    "from transformers.modeling_utils import SequenceSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "train = create_folds(train, num_splits=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvDataset(Dataset):\n",
    "    def __init__(self, df_data):\n",
    "        self.data = copy.deepcopy(df_data)\n",
    "        self.data['excerpt'] = self.data['excerpt'].map(lambda x : x.replace('\\n', ' '))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        if 'target' in self.data.columns:\n",
    "            self.targets = self.data.target.values.tolist()\n",
    "            return {'excerpts': self.excerpts[index], 'targets': self.targets[index]}\n",
    "        else:\n",
    "            return {'excerpts': self.excerpts[index]}\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, tokenizer, max_len=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __call__(self, batch):\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        data_str =[item['excerpts'] for item in batch]\n",
    "        data_batch = self.tokenizer.batch_encode_plus(data_str, \n",
    "                                                        padding='longest', \n",
    "                                                        max_length=self.max_len,\n",
    "                                                        truncation=True, \n",
    "                                                        return_tensors='pt')\n",
    "        \n",
    "        if 'targets' in batch[0]:\n",
    "            targets =torch.tensor([item['targets'] for item in batch])#,dtype=torch.double)\n",
    "            \n",
    "            return {'input_ids': data_batch.input_ids,\n",
    "                'token_type_ids': data_batch.token_type_ids,\n",
    "                'attention_mask': data_batch.attention_mask,\n",
    "                'label':targets\n",
    "                   }\n",
    "        \n",
    "\n",
    "\n",
    "        return {'input_ids': data_batch.input_ids,\n",
    "                'token_type_ids': data_batch.token_type_ids,\n",
    "                'attention_mask': data_batch.attention_mask}\n",
    "        \n",
    "\n",
    "def get_dataloader(dataset, batch_generator, batch_size=4, shuffle=True):\n",
    "    data_loader = DataLoader(dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=shuffle, \n",
    "                             collate_fn=batch_generator,\n",
    "                             num_workers=4,\n",
    "                             pin_memory=True)\n",
    "    \n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n",
    "\n",
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.clmodel = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        self.sequence_summary = SequenceSummary(config)  \n",
    "        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        #self.classifier = RobertaClassificationHead(config)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.clmodel(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        #logits = self.classifier(sequence_output)\n",
    "        output = self.sequence_summary(sequence_output)\n",
    "        logits = self.regressor(output)\n",
    "\n",
    "    \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # regression task\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "            #loss = loss_fn(logits, labels.view(-1))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lamb(Optimizer):\n",
    "    # Reference code: https://github.com/cybertronai/pytorch-lamb\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0,\n",
    "        clamp_value: float = 10,\n",
    "        adam: bool = False,\n",
    "        debias: bool = False,\n",
    "    ):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if eps < 0.0:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 0: {}'.format(betas[0])\n",
    "            )\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 1: {}'.format(betas[1])\n",
    "            )\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(\n",
    "                'Invalid weight_decay value: {}'.format(weight_decay)\n",
    "            )\n",
    "        if clamp_value < 0.0:\n",
    "            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.clamp_value = clamp_value\n",
    "        self.adam = adam\n",
    "        self.debias = debias\n",
    "\n",
    "        super(Lamb, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    msg = (\n",
    "                        'Lamb does not support sparse gradients, '\n",
    "                        'please consider SparseAdam instead'\n",
    "                    )\n",
    "                    raise RuntimeError(msg)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Paper v3 does not use debiasing.\n",
    "                if self.debias:\n",
    "                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n",
    "                    bias_correction /= 1 - beta1 ** state['step']\n",
    "                else:\n",
    "                    bias_correction = 1\n",
    "\n",
    "                # Apply bias to lr to avoid broadcast.\n",
    "                step_size = group['lr'] * bias_correction\n",
    "\n",
    "                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n",
    "\n",
    "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
    "                if group['weight_decay'] != 0:\n",
    "                    adam_step.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                adam_norm = torch.norm(adam_step)\n",
    "                if weight_norm == 0 or adam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / adam_norm\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = adam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "                if self.adam:\n",
    "                    trust_ratio = 1\n",
    "\n",
    "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "    # differential learning rate and weight decay\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    learning_rate = 5e-5\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.01, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.01, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.01, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.clmodel.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"clmodel\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_optimizer = list(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(model_name='../input/xlnet-large-cased/', num_labels=1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('xlnet-large-cased')\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    \n",
    "    ### add below ###\n",
    "    config.update({ \n",
    "        \"attention_probs_dropout_prob\": 0.0,\n",
    "        \"hidden_dropout_prob\": 0.0\n",
    "        })\n",
    "    \n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_optimizer(model, optimizer_name=\"AdamW\"):\n",
    "    optimizer_grouped_parameters = get_optimizer_params(model)\n",
    "    kwargs = {\n",
    "            'lr':5e-5,\n",
    "            'weight_decay':0.01,\n",
    "            # 'betas': (0.9, 0.98),\n",
    "            # 'eps': 1e-06\n",
    "    }\n",
    "    if optimizer_name == \"LAMB\":\n",
    "        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        from torch.optim import Adam\n",
    "        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n",
    "\n",
    "def make_scheduler(optimizer, decay_name='linear', t_max=None, warmup_steps=None):\n",
    "    if decay_name == 'step':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=[30, 60, 90],\n",
    "            gamma=0.1\n",
    "        )\n",
    "    elif decay_name == 'cosine':\n",
    "        scheduler = lrs.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=t_max\n",
    "        )\n",
    "    elif decay_name == \"cosine_warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    elif decay_name == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps, \n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "    fold=0\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_set=csvDataset(data)\n",
    "    train_generator = BatchGenerator(tokenizer)\n",
    "    train_loader = get_dataloader(train_set,train_generator,batch_size)\n",
    "    \n",
    "    valid_set=csvDataset(valid_set)\n",
    "    valid_generator = BatchGenerator(tokenizer)\n",
    "    valid_loader = get_dataloader(valid_set,valid_generator,batch_size//2)\n",
    "    \n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, log_interval=1, evaluate_interval=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.log_interval = log_interval\n",
    "        self.evaluate_interval = evaluate_interval\n",
    "        self.evaluator = Evaluator(self.model)\n",
    "        self.gradient_accumulation_steps = 4\n",
    "\n",
    "    def train(self, train_loader, valid_loader, epoch, \n",
    "              result_dict, tokenizer, fold):\n",
    "        count = 0\n",
    "        self.model.train()\n",
    "        device = 'cuda:1'\n",
    "        \n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "            input_ids, attention_mask, token_type_ids, labels = \\\n",
    "                input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "                #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "          \n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            count += labels.size(0)\n",
    "            \n",
    "            loss = loss / self.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if batch_idx % self.gradient_accumulation_steps:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                self.scheduler.step()\n",
    "    \n",
    "\n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                _s = str(len(str(len(train_loader.sampler))))\n",
    "                ret = [\n",
    "                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n",
    "                    'train_loss: {: >4.5f}'.format(loss),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "            \n",
    "            if batch_idx % self.evaluate_interval == 0:\n",
    "                result_dict = self.evaluator.evaluate(\n",
    "                    valid_loader, \n",
    "                    epoch, \n",
    "                    result_dict, \n",
    "                    tokenizer\n",
    "                )\n",
    "                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n",
    "                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n",
    "\n",
    "        result_dict['train_loss'].append(loss)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def worst_result(self):\n",
    "        ret = {\n",
    "            'loss':float('inf'),\n",
    "            'accuracy':0.0\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    def result_to_str(self, result):\n",
    "        ret = [\n",
    "            'epoch: {epoch:0>3}',\n",
    "            'loss: {loss: >4.2e}'\n",
    "        ]\n",
    "        for metric in self.evaluation_metrics:\n",
    "            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n",
    "        return ', '.join(ret).format(**result)\n",
    "\n",
    "    def save(self, result):\n",
    "        with open('result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "    \n",
    "    def load(self):\n",
    "        result = self.worst_result\n",
    "        if os.path.exists('result_dict.json'):\n",
    "            with open('result_dict.json', 'r') as f:\n",
    "                try:\n",
    "                    result = json.loads(f.read())\n",
    "                except:\n",
    "                    pass\n",
    "        return result\n",
    "\n",
    "    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n",
    "        device = 'cuda:1'        \n",
    "        losses = AverageMeter()\n",
    "\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "                input_ids, attention_mask, token_type_ids, labels = \\\n",
    "                input_ids.to(device), attention_mask.to(device), token_type_ids.to(device), labels.to(device)\n",
    "                \n",
    "              \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] valid_loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "epochs = 8\n",
    "max_len = 1024\n",
    "batch_size = 16\n",
    "\n",
    "#model, tokenizer = make_model(model_name='xlnet-large-cased', num_labels=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "#model = XLNetForSequenceClassification.from_pretrained('xlnet-large-cased', num_labels = 1)\n",
    "#model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels = 1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlnet-large-cased')\n",
    "device= \"cuda:1\"\n",
    "#model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#config = AutoConfig.from_pretrained('roberta-base')\n",
    "# config.update({'num_labels':1})\n",
    "# model = CommonLitModel('roberta-base', config)\n",
    "model_name = 'xlnet-large-cased'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.update({'num_labels':1})\n",
    "model = CommonLitModel(model_name, config=config)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing Last 2 Layers ...\n",
      "Done.!\n"
     ]
    }
   ],
   "source": [
    "reinit_layers = 2\n",
    "_model_type = 'clmodel'\n",
    "_pretrained_model = 'roberta-base'\n",
    "if reinit_layers > 0:\n",
    "    print(f'Reinitializing Last {reinit_layers} Layers ...')\n",
    "    encoder_temp = getattr(model, _model_type)\n",
    "    for layer in encoder_temp.encoder.layer[-reinit_layers:]:\n",
    "        for module in layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "    print('Done.!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [24717, 4, 3], 'token_type_ids': [0, 0, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('model5.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "    fold=0\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_set=csvDataset(train_set)\n",
    "    train_generator = BatchGenerator(tokenizer)\n",
    "    train_loader = get_dataloader(train_set,train_generator,batch_size)\n",
    "    \n",
    "    valid_set=csvDataset(valid_set)\n",
    "    valid_generator = BatchGenerator(tokenizer)\n",
    "    valid_loader = get_dataloader(valid_set,valid_generator,batch_size//2)\n",
    "    \n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = make_loader(\n",
    "    train, tokenizer, max_len=max_len,\n",
    "    batch_size=batch_size, fold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\", line 242, in __getattr__\n    return self.data[item]\nKeyError: 'token_type_ids'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-70-7404f9948c45>\", line 37, in __call__\n    'token_type_ids': data_batch.token_type_ids,\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\", line 244, in __getattr__\n    raise AttributeError\nAttributeError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-0fd2476fd2ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\", line 242, in __getattr__\n    return self.data[item]\nKeyError: 'token_type_ids'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 198, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-70-7404f9948c45>\", line 37, in __call__\n    'token_type_ids': data_batch.token_type_ids,\n  File \"/home/centos/anaconda3/envs/jisu_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\", line 244, in __getattr__\n    raise AttributeError\nAttributeError\n"
     ]
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 16\n",
    "BATCH_SIZE = 8\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "                                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "        input_ids, attention_mask, labels = batch_data['input_ids'], \\\n",
    "        batch_data['attention_mask'], batch_data['label']\n",
    "        input_ids, attention_mask, labels = \\\n",
    "            input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # preds = preds.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        prediction = outputs[1].cpu().detach().numpy()\n",
    "        accuracy=mean_squared_error(labels,prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        counter = counter + 1\n",
    "\n",
    "    return acc / counter, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evluate_epoch(model, data_loader, device):\n",
    "\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = 0\n",
    "    counter = 0\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(data_loader):\n",
    "\n",
    "        input_ids, attention_mask, labels = batch_data['input_ids'], \\\n",
    "        batch_data['attention_mask'], batch_data['label']\n",
    "        input_ids, attention_mask, labels = \\\n",
    "            input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            #token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # preds = preds.cpu().detach().numpy()\n",
    "        labels = labels.cpu().detach().numpy()\n",
    "        prediction = outputs[1].cpu().detach().numpy()\n",
    "        accuracy=mean_squared_error(labels,prediction)\n",
    "\n",
    "        acc += accuracy\n",
    "        counter = counter + 1\n",
    "\n",
    "    return acc / counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "----------\n",
      "Train loss 0.6325833246531621 Valid loss 0.3086862495877373\n",
      "Epoch 2/16\n",
      "----------\n",
      "Train loss 0.27090589851665664 Valid loss 0.3561110838618077\n",
      "Epoch 3/16\n",
      "----------\n",
      "Train loss 0.19169292173964878 Valid loss 0.2647916595385948\n",
      "Epoch 4/16\n",
      "----------\n",
      "Train loss 0.13087898429850459 Valid loss 0.2808031904655443\n",
      "Epoch 5/16\n",
      "----------\n",
      "Train loss 0.09727156159638518 Valid loss 0.3122894480614595\n",
      "Epoch 6/16\n",
      "----------\n",
      "Train loss 0.07133061402070691 Valid loss 0.3017549442363457\n",
      "Epoch 7/16\n",
      "----------\n",
      "Train loss 0.061249757736501556 Valid loss 0.26380499649110817\n",
      "Epoch 8/16\n",
      "----------\n",
      "Train loss 0.050833152725138296 Valid loss 0.28153270839805333\n",
      "Epoch 9/16\n",
      "----------\n",
      "Train loss 0.03911829486646703 Valid loss 0.2689841369193205\n",
      "Epoch 10/16\n",
      "----------\n",
      "Train loss 0.036853269542711724 Valid loss 0.26640812895247634\n",
      "Epoch 11/16\n",
      "----------\n",
      "Train loss 0.029461589727130994 Valid loss 0.2454002123903221\n",
      "Epoch 12/16\n",
      "----------\n",
      "Train loss 0.031125376401075596 Valid loss 0.2530694269693234\n",
      "Epoch 13/16\n",
      "----------\n",
      "Train loss 0.024468595427657728 Valid loss 0.2572309657878859\n",
      "Epoch 14/16\n",
      "----------\n",
      "Train loss 0.020065007547677403 Valid loss 0.2599889623971892\n",
      "Epoch 15/16\n",
      "----------\n",
      "Train loss 0.01947154326718563 Valid loss 0.24866243357389745\n",
      "Epoch 16/16\n",
      "----------\n",
      "Train loss 0.015512915410127648 Valid loss 0.2534704481212186\n",
      "CPU times: user 7min 51s, sys: 3min 42s, total: 11min 34s\n",
      "Wall time: 11min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 999\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_loader,     \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler\n",
    "    )\n",
    "    valid_loss = evluate_epoch(\n",
    "        model,\n",
    "        valid_loader,     \n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(f'Train loss {train_acc} Valid loss {valid_loss}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)    \n",
    "\n",
    "    if valid_loss < best_accuracy:\n",
    "        torch.save(model.state_dict(), \"model0.bin\")\n",
    "        best_accuracy = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loberta\n",
    "# Large model 0.24114447321132035\n",
    "# Base model also get 0.249"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_csv('input/test.csv')\n",
    "\n",
    "# test_set=csvDataset(test)\n",
    "# generator = BatchGenerator(tokenizer)\n",
    "# test_loader = get_dataloader(test_set,generator,batch_size)\n",
    "\n",
    "# input_ids, attention_mask, token_type_ids = test_batch['input_ids'], \\\n",
    "# test_batch['attention_mask'], test_batch['token_type_ids']\n",
    "# input_ids, attention_mask, token_type_ids = \\\n",
    "#     input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask= test_batch['input_ids'], \\\n",
    "test_batch['attention_mask']\n",
    "input_ids, attention_mask = \\\n",
    "    input_ids.to(device), attention_mask.to(device)\n",
    "    #input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.1450],\n",
       "        [-0.0205],\n",
       "        [-1.3368],\n",
       "        [-1.3012],\n",
       "        [-0.6730],\n",
       "        [-0.8062],\n",
       "        [-1.4579],\n",
       "        [-0.0270],\n",
       "        [-0.2827],\n",
       "        [-2.2349],\n",
       "        [-0.3861],\n",
       "        [-0.5837],\n",
       "        [-1.8859],\n",
       "        [-1.2470],\n",
       "        [ 0.9654],\n",
       "        [-3.3354]], device='cuda:1', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1969, -0.0868, -1.2149, -1.2125, -0.8335, -0.7310, -1.3928,  0.0215,\n",
       "        -0.2370, -2.2534, -0.4163, -0.3716, -1.7088, -1.0768,  1.0909, -3.3092])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassificationOutput(loss=None, logits=tensor([[-1.8539],\n",
       "        [-1.7061],\n",
       "        [-1.5374],\n",
       "        [-1.6646],\n",
       "        [-2.0674],\n",
       "        [-0.8819],\n",
       "        [-0.0977],\n",
       "        [-2.3524],\n",
       "        [-0.1410],\n",
       "        [-0.3518],\n",
       "        [ 1.2315],\n",
       "        [-2.3641],\n",
       "        [ 1.5429],\n",
       "        [ 1.4368],\n",
       "        [-0.1662],\n",
       "        [-1.1045]], device='cuda:1', grad_fn=<AddmmBackward>), mems=None, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    token_type_ids=token_type_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jisu_env",
   "language": "python",
   "name": "jisu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
